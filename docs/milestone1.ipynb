{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Problem Description\n",
    "<!-- the problem the software solves -->\n",
    "This library solves the computational technique of automatic differentiation: a process to compute derivatives numerically within accurate machine approximation. This task is also called algorithmic differentiation, computational differentiation, or auto-differentation.\n",
    "\n",
    "Computing derivatives can be an easy task for a human, simply apply a few rules here and there and some specific functional derivatives. If complexity is low, a human can compute a derivative in one step. A computer, however, runs computations one at a time (ignoring parallel computing), collecting the information about the small computations in memory. That is, calaulcations are made step by step, the result being applied to the subsequent computation needed in the grand scheme of computing the overall derivative. A computer must either be given the values to make a direct computation, or the values that allow it to perform minor computations with those values and aggregate them with elementary operations like multiplcation, division, addition, or subtraction.\n",
    "\n",
    "Many algorithms are *optimizational*, which requires the use such derivatives. When dealing with problems with more than one dimension, we must adapt the simple optimization problem into a multi-dimensional optimization problem. Derivatives and algorithms that use them are utilities, primaily used for their contribution to a bigger project rather than instrinsic value.\n",
    "\n",
    "## Motivations\n",
    "<!-- why it's important to solve that problem -->\n",
    "Optimization is everywhere today, as it aims to solve a problem according to some constraint to the best of its ability. Minimizing the error of a function for *machine learning* frequently requires multi-dimensional optimization. There are a few more specific examples listed below.\n",
    "\n",
    "- *Neural networks* require the adjustement of a vector of weights per layer, therefore requiring us to optimize with respect to some standard (function).\n",
    "- Using *hueristic artificial intelligence methods* like *hill climbing* or *partical swarm* require the adjustment of -- for example purposes -- a ball dropped into a plane and moved in the direction (x, y) that gets the ball to the target point the fastest. \n",
    "- Problems in *physics* and *material science* requires the analysis of functions that rely on time, position, speed, and other problem-specific metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "<!-- (Brief) give the essential ideas of mathematical ideas and concepts (e.g. the chain rule, the graph structure of calculations, elementary functions, etc) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Guide\n",
    "\n",
    "<!-- How do you envision that a user will interact with your package? What should they import? How can they instantiate AD objects? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "<!-- \n",
    "    What will the directory structure look like?\n",
    "    What modules do you plan on including? What is their basic functionality?\n",
    "    Where will your test suite live?\n",
    "    How will you distribute your package (e.g. PyPI with PEP517/518 or simply setuptools)?\n",
    "    Other considerations?\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    " ## Foward-Mode AD\n",
    " <!-- \n",
    "    What are the core data structures?\n",
    "    What classes will you implement?\n",
    "    What method and name attributes will your classes have?\n",
    "    What external dependencies will you rely on?\n",
    "    How will you deal with elementary functions like sin, sqrt, log, and exp (and all the others)?\n",
    " -->\n",
    "\n",
    " <!-- consider a variety of use cases. For example, don't limit your design to scalar functions of scalar values. People may want to use it for Newton's method -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Licensing\n",
    "\n",
    " The MIT license is permissive and only requires the maintence of copyright and license notices. Unlike other licenses, no update notice is required. Automatic Differentiation is not a new method, thereby not needing a patent. Our library simply aims to make the user's project easier by importing a pre-defined solver rather than build one from scratch.\n",
    "\n",
    " The library and its usage comes as is, without warranty. Automatic differentiation is a mathematical solver, it is likely going to be embedded in a user's larger project, which is supported by this as well as its high license compatability. Limited restrictions on use. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
