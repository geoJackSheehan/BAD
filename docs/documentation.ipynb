{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to our BAD (Basic Auto Differentiator) Package\n",
    "\n",
    "## Problem Description\n",
    "<!-- the problem the software solves -->\n",
    "This library solves the computational technique of automatic differentiation: a process to compute derivatives numerically within accurate machine approximation. This task is also called algorithmic differentiation, computational differentiation, auto-differentiation, or the acronym *AD*. The heart of AD lies in breaking down compounded functions to their basic components: arithmetic operations and elementary functions. The derivatives for these elementary functions are well known. Finally, the chain rule is applied to these operations and derivatives of any order can be computed.\n",
    "\n",
    "If complexity is low, a human can compute a derivative in one step using some memorized and nuanced rules. Often what is produced is *symbolic*, which is then evaluated for a numerical output for a given input. Computer cannot handle symbolic forms in the same way. They must either be given the values to make a direct computation, or the values that allow it to perform minor computations with those values and aggregate them with elementary operations like multiplication, division, addition, or subtraction. These are then compounded to form the overall derivative. While the approach is more sequential, the process is the same.\n",
    "\n",
    "## Motivations\n",
    "<!-- why it's important to solve that problem -->\n",
    "Many algorithms today are *optimizational*: aiming to solve a problem according to constraint(s) to the best of its ability. Optimization requires the use of such derivatives. When dealing with problems with more than one dimension, we must adapt the simple optimization problem into a multi-dimensional optimization problem. Minimizing the error of a function for *machine learning* frequently requires multi-dimensional optimization. Besides outperforming humans for complex functions, computers can also easily compute derivatives for 2-D `numpy` array inputs, outputting Jacobian matrices of results. Computationally intensive problems as listed below require derivatives within a larger pipeline, and must be self contained. AD is necessary for this purpose.\n",
    "\n",
    "- Generally used to model *1D flow* and *heat transfer* problems.\n",
    "- Generally used to optimize the *aerodynamic* properties of a body.\n",
    "- Problems in *physics* and *material science* requires the analysis of functions that rely on time, position, speed, and other problem-specific metrics.\n",
    "- *Adjoint Differentiation* in financial risk management is exactly this process.\n",
    "- Using *heuristic artificial intelligence methods* like *hill climbing* require the adjustment of -- for example purposes -- a ball dropped into a plane and moved in the direction that gets the ball to the target point the fastest. \n",
    "- Can solve *Ordinary Differential Equations* with a removable singularity.\n",
    "- *Neural networks* require the adjustment of a vector of weights per layer, therefore requiring us to optimize with respect to some standard (function).\n",
    "- NASA used AD for *computational fluid dynamics* with a larger goal of modeling flight and expected turbulence of a given space-craft design.\n",
    "- General Electric used AD to automate the study of *unexpected satellite motion* due to differences in Earth's gravitational flow.\n",
    "- Northwestern University published work on using AD as a framework to *reduce diffractions in microscopic imaging*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "<!-- (Brief) give the essential ideas of mathematical ideas and concepts (e.g. the chain rule, the graph structure of calculations, elementary functions, etc) -->\n",
    "\n",
    "Automatic differentiation computes the derivative of a function by solving it as a collection of the derivatives of its components. These individual components are elementary functions, connected by known, simple operators. The purpose behind this separation is that the individual functions can be easily solved using pre-established rules. The automatic differentiator can then solve any function made up of these components. \n",
    "\n",
    "## Elementary Functions\n",
    "Some examples of elementary functions include $e^x$, $\\log(x)$, $\\sin(x)$, and $\\cos(x)$. Some examples of elementary arithmetic operations include addition, subtraction, multiplication, and division. Automated differentiation uses the chain rule to split the given function into these elementary operators.\n",
    "\n",
    "## Derivatives\n",
    "A derivative describes the slope of a given function generally (produces another function denoted $f'$) or at a specific point by plugging in the value of interest into $f'$. In a general form, slopes tell us how much a response variable (usually $y$, which is often written as $f(x)$) changes when the input $x$ does. The best example of this concept is the equation of a line: $z = mx + b$, where $m$ is the slope. In this case, $m$ is a number. When we look at more complicated functions, the slope is often left in its functional representation $f'$ and used by plugging in a point later for the slope at a specific point.\n",
    "\n",
    "Slopes are the main component behind AD, since we are interested in how the given function behaves -- how our response variable changes -- as we \"look\" in isolated directions. If we have a function $f$ described by two variables $x, y$, we would want to compute two derivatives -- one *with respect* to $x$ while holding $y$ constant (think of it as just another number) and the other *with respect* to $y$ while holding $x$ constant. We can then answer the corresponding two questions of interest -- how much and in what way does the function $f$ change when $x$ does, and how much and in what way does the function $f$ change when $y$ does? \n",
    "\n",
    "## Chain Rule\n",
    "The chain rule applies to any elementary functions with variable input, which occurs often. We will be applying the chain rule repeatedly to our elementary operators. The chain rule can be further explored here.\n",
    "\n",
    "Suppose we have a function $g(f(x))$. Then, the derivative of this function will be\n",
    "\n",
    "$$\\frac{dg}{dx} = \\frac{dg}{df}\\frac{df}{dx}.$$\n",
    "\n",
    "Note that when functions have multiple arguments, $h(f(x), g(x))$, the chain rule can be applied like so\n",
    "\n",
    "$$\\frac{dh}{dx} = \\frac{dh}{df}\\frac{df}{dx} + \\frac{dh}{dg}\\frac{dg}{dx}.$$\n",
    "\n",
    "## Dual Numbers\n",
    "Another important component of automatic differentiation are dual numbers. Dual numbers have a real and dual component, expressed as $ z=a+b\\epsilon $. Here, $a$ is the real part and $b$ is the dual part. The $\\epsilon$ is a non-real number where $ \\epsilon^2=0 $ but $ \\epsilon \\neq 0 $. They are used to track the application of an elementary function on a normal scalar (the real part) and the derivative of that (the dual part) simultaneously for each step as part of our data structure.\n",
    "\n",
    "## Forward Mode\n",
    "In a general sense, forward mode is an iterative evaluation of the chain rule. The evaluation trace is the outline of this path, step by step. At each step, we record the elementary operation, the directional derivative, and the output when solving for some value. For complex functions, computer numerical storage is beneficial because this method requires tracking all of these operations. Forward mode is most efficient when a potential function has significantly more outputs than inputs.\n",
    "\n",
    "## Reverse Mode\n",
    "Another feature that we implemented in our auto differentiation is reverse mode AD. Reverse mode AD traverses the chain rule from the outside to inside. Reverse mode AD still relies on the computational graph used for forward mode AD, but computes the gradient by a reverse traversal of the computational graph. In reverse mode AD, instead of storing all of the derivative information at each node, only the partial derivatives of nodes relative to its children are being stored. Like forward mode, we overload operations, implementing addition, subtraction, multiplication, etc. In most cases, using reverse mode AD is less computationally expensive than forward mode AD in machine learning. \n",
    "\n",
    "## Computational Graph and Trace Example\n",
    "<img src=\"./compgraph.png\" alt=\"Alternative text\" />\n",
    "\n",
    "<!-- \n",
    "Note: We should add a computation graph and notes about forward mode (and reverse mode?).\n",
    "Note: We should make a table(?) of all the accepted functions and their derivatives/how they will be used? ($ a+b $, $ a-b $, $ ab $, $ \\frac{a}{b} $, $ a^{b} $, $ sin(a) $, $ cos(a) $, $ tan(a) $, $ ln(a) $)\n",
    "\n",
    "dkim: added brief notes about elementary functions, i think we should add a forward mode/reverse mode section as well as computational graphs for each\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Guide for the BAD Package\n",
    "\n",
    "<!-- How do you envision that a user will interact with your package? What should they import? How can they instantiate AD objects? -->\n",
    "\n",
    "## Installation\n",
    " A user can install the package with:\n",
    "\n",
    "``python -m pip install -i https://test.pypi.org/simple/ bad-package``\n",
    "\n",
    " Note: Our package will utilize NumPy.\n",
    "\n",
    "## Importing\n",
    " Once successfully installed, a user can import this package in their Python script like so:\n",
    "    \n",
    "``import bad_package``\n",
    "\n",
    "### Instantiating BAD Objects\n",
    " Here is a quick demo of how our BAD package can be used.\n",
    "\n",
    "```python\n",
    "# import our package:\n",
    "from bad_package.ad_interface import *\n",
    "from bad_package.elementary_functions import *\n",
    "from bad_package.fad import *\n",
    "import numpy as np\n",
    "\n",
    "# create a function to be evaluated at x=2:\n",
    "def my_fun(x):\n",
    "    return 2*x + 5\n",
    "\n",
    "x = np.array([2])\n",
    "\n",
    "# instantiate AD\n",
    "ad = AutoDiff(my_fun, x)\n",
    "\n",
    "# Primal Trace (function evaluated at provided value)\n",
    "print(f'Primal: {ad.get_primal()}')\n",
    "\n",
    "# Jacobian\n",
    "print(f'Jacobian: {ad.get_jacobian()}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Software Organization\n",
    "\n",
    "<!-- \n",
    "    What will the directory structure look like?\n",
    "    What modules do you plan on including? What is their basic functionality?\n",
    "    Where will your test suite live?\n",
    "    How will you distribute your package (e.g. PyPI with PEP517/518 or simply setuptools)?\n",
    "    Other considerations?\n",
    " -->\n",
    " \n",
    "## Directory Structure\n",
    "```\n",
    "|team23\n",
    "|—— docs\n",
    "|  |—— README.md\n",
    "|  |—— compgraph.png\n",
    "|  |—— documentation.ipynb\n",
    "|  |—— milestone1.ipynb\n",
    "|  |—— milestone2.ipynb\n",
    "|  |—— milestone2_progress.ipynb\n",
    "|—— src\n",
    "|  |—— bad_package\n",
    "|  |  |—— __init__.py\n",
    "|  |  |—— __main__.py\n",
    "|  |  |—— elementary_functions.py\n",
    "|  |  |—— fad.py\n",
    "|  |  |—— interface.py\n",
    "|  |  |—— rad.py\n",
    "|——tests\n",
    "|  |—— check_coverage.sh\n",
    "|  |—— run_tests.sh\n",
    "|  |—— test_derivs.py\n",
    "|  |—— test_elementary_functions.py\n",
    "|  |—— test_fad.py\n",
    "|  |—— test_interface.py\n",
    "|  |—— test_rad.py\n",
    "|—— LICENSE\n",
    "|—— README.md\n",
    "|—— pyproject.toml\n",
    "```\n",
    "## Modules & Implementation\n",
    "<!-- \n",
    "Note: What modules do you plan on including? What is their basic functionality?\n",
    " -->\n",
    "\n",
    "### Dependent and General Core Structures\n",
    "- Python Types: `int`, `float`, `string`, `list`\n",
    "- Numpy: `array`\n",
    "\n",
    "### Basic Operator Overloading/Elementary Functions\n",
    "- For all `numpy` functions or constants, we will rewrite and manually solve them so the user doesn’t have to use `np.<function or constant>`.\n",
    "- We will overload our basic operators to solve for the derivative of real and dual components and to work with each method listed above.\n",
    "\n",
    "### Library Dependence\n",
    "- `Numpy`\n",
    "   - $\\log$, sqrt, $\\exp$\n",
    "   - $\\sin$, $\\cos$, $\\tan$\n",
    "   - $\\csc$, $\\sec$, $\\cot$\n",
    "   - $\\sin^{-1}$, $\\cos^{-1}$, $\\tan^{-1}$\n",
    "   - $\\sinh$, $\\cosh$, $\\tanh$\n",
    "   - $\\sinh^{-1}$, $\\cosh^{-1}$, $\\tanh^{-1}$\n",
    "   - e, $\\pi$\n",
    "\n",
    "- `pytest`\n",
    "   - For internal testing and code coverage only.\n",
    "\n",
    " \n",
    "<!--\n",
    "-)\n",
    "-) Ok this is just me writing for ideas. We need a dual class, and maybe a function class? to read in the user input?\n",
    "-) For methods we will have: __add__, __sub__, __mul__, __div__, any others? For attributes we will have: __init__, derivative, seed_vector, variable?\n",
    "-) numpy, any others? (math, scipy, etc)\n",
    "-) like he did in class in the simple automatic differentiator, we should just manually define these ahead of time \n",
    "-->\n",
    "\n",
    " \n",
    " \n",
    "### `bad_package`\n",
    "Located one level below the root directory, packaged within `src` directory.\n",
    "\n",
    "- `__init__`.py\n",
    "    - Initializing the package whenever package or module within the package is being imported. \n",
    "- `__main__.py`\n",
    "    - This file is the environment where top-level code is run, and it is executed by the interpreter whenever `-m` is passed. The `bad_package/__main__.py` can be executed using `python -m bad_package`. \n",
    "- `elementary_functions.py`\n",
    "    - Non-class module containing 19 overloaded `Numpy` functions and constant $\\pi$ and $e$. This re-describes the behavior of basic functions on typical data types (`int`, `float`), custom data structure Dual Number, and custom data structure Reverse Mode. All Dual Number components were calculated using a trace table with V0 = x.real,  DpV0 = x.dual. Thus, the dual component is the analytical derivative of the elementary function acting on a variable `x`.\n",
    "    - Elementary functions are recursive in Dual Number computation, such that we pass the real part and/or dual part to another function which will be handled as scalar input and maintain domain integrity. This ensures all `Numpy` errors are overwritten with custom errors and all data types are validated.\n",
    "    - Domain restrictions on functions are imposed by most restrictive case, respective to component.\n",
    "    - Expectations:\n",
    "        1. Any elementary function is being applied to a single DualNumber instance, ReverseMode instance, or int / float.\n",
    "        2. All DualNumber instances already have float-type real and dual parts.\n",
    "        3. All ReverseMode instances already have float-type real part.\n",
    "        4. NotImplementedError raised if there is (1) unexpected behavior or (2) devs haven't gotten to implementing something yet.\n",
    "        5. TypeError raised if an inappropriate data type was passed from the user.\n",
    "        6. ArithmeticError raised if there was a (generally) mathematically inappropriate calculation about to happen.\n",
    "\n",
    "- `fad.py`\n",
    "    - Contains the main data structure for automatic differentiation, the Dual Number. All computational dunder methods such as addition, subtraction, multiplication, division, power, ect are implemented here. \n",
    "    \n",
    "- `rad.py`\n",
    "    - Contains the main data structure for reverse mode. All computational dunder methods such as addition, subtraction, multiplication, division, power, ect are implemented here. \n",
    "\n",
    "- `interface.py`\n",
    "    - The public interface. A user instantiates an AutoDiff or ReverseAD object from the classes contained within this module, passing pre-constructed function(s) and value(s) to evaluate the function(s) at. Produces a Jacobian which can be accessed using `<AutoDiff>.get_jacobian()` or `<ReverseAD>.get_jacobian()`. Normal functional evaluation can be accessed by `<AutoDiff>.get_primal()`. Computations of the jacobian and primal are done automatically inside the class.\n",
    "\n",
    "## Tests\n",
    "The test suite will be located in the `tests` directory in the package's root directory.\n",
    "\n",
    "- `test_fad.py`\n",
    "    - Checks all overloaded operators for Dual Numbers produce the desired output, including the proper raise statements when given invalid input.\n",
    "    \n",
    "- `test_rad.py`\n",
    "    - Checks all overloaded operators for ReverseMode produce the desired output, including the proper raise statements when given invalid input.\n",
    "\n",
    "- `test_elementary_functions.py`\n",
    "    - Checks all the overloaded Numpy operators and constants. Ensures each of the 19 functions raise proper errors when encountering functional domain restrictions and that the derivative is properly tracked in the dual portion of the Dual Number.  \n",
    "    \n",
    "- `test_interface.py`\n",
    "    - Ensures interface for automatic differentiation for forward and reverse mode functions as intended for scalar and vectorized functions and variables.\n",
    "\n",
    "- `test_derivs.py`\n",
    "    - Agnostic testing script checking derivative computations made by our program with expected output as done by a human and evaluated at a given point. Currently only dealing with certain scalar complex functions.\n",
    "\n",
    "## Distribution\n",
    "The package will be distributed using the Python Package Index, `PyPI`. It will also be available to be cloned through our `team23` [GitHub repository](https://code.harvard.edu/CS107/team23)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Licensing\n",
    "\n",
    " The MIT license is permissive and only requires the maintenance of copyright and license notices. Unlike other licenses, no update notice is required. Automatic Differentiation is not a new method, thereby not needing a patent. Our library simply aims to make the user's project easier by importing a pre-defined solver rather than build one from scratch.\n",
    "\n",
    " The library and its usage comes as is, without warranty. Automatic differentiation is a mathematical solver, it is likely going to be embedded in a user's larger project, which is supported by this as well as its high license compatibility. Limited restrictions on use.\n",
    "\n",
    " This package is completely free and open source. We encourage anyone and everyone to study and experiment as a way to learn and improve. Any misuse for academic gain (cheating) or malicious intent will not be tolerated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Getting Started\n",
    "\n",
    "## Installation via PyPI\n",
    "\n",
    "1. Ensure your Python version is 3.8 or greater.\n",
    "\n",
    "2. Ensure `pip` and `setuptools` are installed and up to date.\n",
    "\n",
    "    ```\n",
    "    # Linux (Ubuntu), MacOS\n",
    "    python -m ensurepip --upgrade\n",
    "    python -m pip install --upgrade pip\n",
    "\n",
    "    python -m pip install setuptools\n",
    "    ```\n",
    "\n",
    "3. This package depends on `numpy` version 1.21.0 and greater. Please ensure it is installed and up to date.\n",
    "\n",
    "    ```\n",
    "    # Linux (Ubuntu), MacOS\n",
    "    python -m pip install numpy\n",
    "    python -m pip install --upgrade numpy\n",
    "    ```\n",
    "\n",
    "4. Pip install from the test PyPI\n",
    "\n",
    "    ```\n",
    "    python -m pip install https://test.pypi.org/simple/ bad_package\n",
    "    ```\n",
    "\n",
    "## Installation via GitHub\n",
    "  \n",
    "1. Clone the package repository to folder. Make sure to set up a SSH key beforehand.\n",
    "\n",
    "    ```\n",
    "    mkdir bad_package\n",
    "    cd bad_package\n",
    "    git clone git@code.harvard.edu:CS107/team23.git\n",
    "    cd team23\n",
    "    ```\n",
    "<br>\n",
    "    \n",
    "2. Install virtualenv on your machine if not already installed. \n",
    "\n",
    "    ```\n",
    "    pip install virtualenv\n",
    "    ```\n",
    "<br>  \n",
    "    \n",
    "3. Create virtual environment\n",
    "\n",
    "    ```\n",
    "    virtualenv cs107\n",
    "    ```\n",
    "<br>  \n",
    "  \n",
    "4. Activate the new virutal environment\n",
    "\n",
    "    Mac OS or Linux:\n",
    "    \n",
    "    ```python\n",
    "    source cs107/bin/activate\n",
    "    ```\n",
    "     \n",
    "    Windows:\n",
    "    \n",
    "    ```python\n",
    "    cs107\\Scripts\\activate    \n",
    "    ```\n",
    "<br>\n",
    "\n",
    "5. Install package and its requirements \n",
    "\n",
    "    ```python\n",
    "    pip install ./\n",
    "    ```    \n",
    "<br>\n",
    "\n",
    "6. To deactivate virtual environment\n",
    "\n",
    "    ```python\n",
    "    deactivate\n",
    "    ```        \n",
    "    \n",
    "## Using Forward Mode\n",
    "\n",
    "### Import modules\n",
    "\n",
    "```python\n",
    ">>> from bad_package.interface import *\n",
    ">>> from bad_package.elementary_functions import *\n",
    ">>> from bad_pacakge.fad import *\n",
    ">>> import numpy as np\n",
    "```\n",
    "\n",
    "### How to use Forward Mode\n",
    "\n",
    "#### Scalar \n",
    "\n",
    "```python\n",
    "# User defines the function that they want to optimize. \n",
    ">>> def scalar(x):\n",
    ">>>     return 4*x + 3\n",
    "\n",
    "# User creates a 1D numpy array of initial value for input to the function they want to optimize\n",
    ">>> x = np.array([2])\n",
    "\n",
    "# User instantiate AutoDiff class\n",
    ">>> ad = AutoDiff(scalar, x)\n",
    "\n",
    "# User can call the primal trace and jacobian matrix\n",
    ">>> print(f'Primal: {ad.get_primal()}')\n",
    "Primal: 11\n",
    ">>> print(f'Tangent: {ad.get_jacobian()}')\n",
    "Tangent: [4]\n",
    "```\n",
    "\n",
    "#### Vector\n",
    "\n",
    "```python\n",
    "# User defines the function that they want to optimize. \n",
    ">>> def vector(x):\n",
    ">>>     return x[0]**2 + 3*x[1] + 5\n",
    "\n",
    "# User creates a N-D numpy array of initial value for input to the function they want to optimize\n",
    ">>> x = np.array([1, 2])\n",
    "\n",
    "# User instantiate AutoDiff class\n",
    ">>> ad = AutoDiff(vector, x)\n",
    "\n",
    "# User can call the primal trace and jacobian matrix\n",
    ">>> print(f'Primal: {ad.get_primal()}')\n",
    "Primal: 12\n",
    ">>> print(f'Tangent: {ad.get_jacobian()}')\n",
    "Tangent: [2, 3]\n",
    "```\n",
    "\n",
    "\n",
    "## Using Reverse Mode\n",
    "\n",
    "### Import modules\n",
    "\n",
    "```python\n",
    ">>> from bad_package.interface import *\n",
    ">>> from bad_package.elementary_functions import *\n",
    ">>> from bad_pacakge.rad import *\n",
    ">>> import numpy as np\n",
    "```\n",
    "\n",
    "### How to use Reverse Mode\n",
    "\n",
    "#### Scalar \n",
    "\n",
    "```python\n",
    "# User defines the function that they want to optimize. \n",
    ">>> def scalar(x):\n",
    ">>>     return 4*x + 3\n",
    "\n",
    "# User creates a 1D numpy array/float/scalar of initial value for input to the function they want to optimize\n",
    ">>> x = np.array([2]) \n",
    "\n",
    "# User instantiate ReverseAD class\n",
    ">>> rm = ReverseAD(scalar, x)\n",
    "\n",
    "# User can call the jacobian matrix\n",
    ">>> print(f'Jacobian: {rm.get_jacobian()}')\n",
    "Jacobian: [4]\n",
    "```\n",
    "\n",
    "#### Vector\n",
    "\n",
    "```python\n",
    "# User defines the function that they want to optimize. \n",
    ">>> def vector(x):\n",
    ">>>     return x[0]**2 + 3*x[1] + 5\n",
    "\n",
    "# User creates a N-D numpy array of initial value for input to the function they want to optimize\n",
    ">>> x = np.array([1, 2])\n",
    "\n",
    "# User instantiate ReverseAD class\n",
    ">>> rm = ReverseAD(vector, x)\n",
    "\n",
    "# User can call the jacobian matrix\n",
    ">>> print(f'Jacobian: {rm.get_jacobian()}')\n",
    "Jacobian: [2, 3]\n",
    "```\n",
    "\n",
    "#### Vector 2 \n",
    "\n",
    "\n",
    "```python\n",
    "# User defines the functions that they want to optimize. \n",
    ">>> def vector1(x):\n",
    ">>>     return (5*x + 50)/(2*x**2)\n",
    "\n",
    ">>> def vector2(x):\n",
    ">>>     return 10 + 2*x\n",
    "\n",
    ">>> func = np.array([vector1, vector2])\n",
    "\n",
    "# User creates a 1D numpy array/float/scalar of initial value for input to the function they want to optimize\n",
    ">>> x = np.array([5])\n",
    "\n",
    "# User instantiate ReverseAD class\n",
    ">>> rm = ReverseAD(vector, x)\n",
    "\n",
    "# User can call the jacobian matrix\n",
    ">>> print(f'Jacobian: {rm.get_jacobian()}')\n",
    "Jacobian: [-0.5, 2]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "As with any development, the first version is never the best or final version. New methods are created, better workflow is brainstormed, and extensions are thought up. Our package meets all functionality requirements and provides additional user flexibility, but due to time constraints prior to initial release we were unable to implement the most pressing upgrades.\n",
    "\n",
    "1. We suspect that minor algorithmic tweaks could be made for more efficient computation and storage, which would improve long-term performance and speed up complex use cases. \n",
    "2. A more robust interface behind the scenes would improve user-facing simplicity and allow for more skilled developers to dive into the world of storage and metadata associated with internal object instantiation.\n",
    "3. We did not use inheritance between forward and reverse objects, but ideally we would like to exploit these and make a base class to be more pythonic.\n",
    "4. Both methods inherently use a computational graph, which can be complicated to a new user. We would like to provide clarity and more learning opportunities to developers through building and storing a graphical version of the computational graph for all AD objects.\n",
    "5. Elementary functions overloads `numpy` operations in a very specific and recursive way to handle functional domain issues such as asymptotes. While the structure has been changed, we suspect there is a more efficient way of handling type-checks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Extentions\n",
    "\n",
    "Beyond the immediate Future Work listed prior, there are more features to add. Most problems and developers that require an automatic differentiation package are working with complex systems and require beyond the first-order derivative. Supporting higher-order derivatives to accommodate a wide breadth of fields like machine learning, artificial intelligence, general ordinary and partial differential equations, and physics is desireable. \n",
    "\n",
    "From personal experience, we know computational cost of automatic differentiation (especially forward) is high. Since the output is the same for forward and reverse mode, we would ideally ask the user to instantiate one agnostic AutoDiff object that internally decides whether computational cost exceeds the memory storage threshold and needs to be done in reverse. This also provides the user with more simplicity.\n",
    "\n",
    "Machine learning and similar fields has exploded, using more complex models and methods every day. Complex multi-variate linear systems require millions of steps, which can take quite some time to run. This raises the need for multi-threaded execution support, which is a staple of the most reputable machine learning and artificial intelligence packages. It would also be a fantastic learning opportunity for those who use our code as an academic piece.\n",
    "\n",
    "Another common problem in machine learning is the vanishing gradient. This often occurs in neural networks, which use reverse mode in back-propagation. With convolution neural networks, deep neural networks, and the like, we would like to adapt this package to handle vanishing gradients by supporting ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broader Impact\n",
    "<!-- The potential broader impacts and implications of your software.\n",
    "How is your software inclusive to the broader community? -->\n",
    "<!-- ANNABEL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As developers of an open-sourced Auto Differentiation package, we care about bringing diversity, equity, and inclusion to the open-source community. We hope that our package can be used by a diverse group of people, including women, people of color, people with disabilities, and an exhaustive list consisting of minorities. We hope we can provide a safe space for motivating others to contribute and point out possible changes that need to be made for this open-sourced package. \n",
    "\n",
    "This open-sourced package may be misused and cause some serious ethical issues. Although this package is an efficient tool to solve complex gradient problems without solving them by hand, prior to using this package, we hope users can spend time understanding the real mathematical implications and uses of taking derivatives and gradients. Our package is primarily used for academic purposes and should not be for sale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Inclusivity Statement\n",
    "<!-- In principle, there should be no barrier whatsoever for other developers to contribute to your code base. In practice, these barriers do exist and can be rather subtle. For example, are there any subtle barriers to underrepresented groups? What about working parents? What about people from different countries or non-native English speakers? Do people from rural communities feel they have something to offer? Carefully think about the code contribution process for your software project. How are pull requests being reviewed and approved? Who is reviewing and approving these requests? Python has a Diversity Statement, but it is fairly generic and contains a lot of boiler plate. Different Python groups may have more concrete policies. Can you do better? -->\n",
    "Team23 is dedicated to creating the software space more inclusive to underrepresented groups. To promote this standard, the BAD package will be released to the open source community welcoming contribution from any person. The core developers of BAD package believe that any individual, regardless of background, is capable of making meaningful contributions within the software community. We welcome _all_ developers that believe in fostering a respectful environment to contributing to our package. To make BAD package more accessible to non-native English speakers, we have plans to translate our documentation to various languages. We welcome any new ideas and feedback to improve the BAD package! Team23 believes that the best work is a product of collaboration of diverse backgrounds.\n",
    "\n",
    "Pull requests will be reviewed blindly among the core developers to mitigate bias. If not approved by a majority vote, relevant supportive feedback will be provided.\n",
    "\n",
    "Any unethical activity under the BAD package that fails to uphold this inclusive standard will not be tolerated. Discrimination is not welcome here.\n",
    "\n",
    "Feel free to contact the team via email for any further questions:\n",
    "- annabelyim@g.harvard.edu\n",
    "- hopeneveux@g.harvard.edu\n",
    "- jacksheehan@g.harvard.edu\n",
    "- sharonkim@g.harvard.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4072ef9a63b419d661cb82457dc87daa7ba7c0e58f8751cc56eacde49ff66d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
